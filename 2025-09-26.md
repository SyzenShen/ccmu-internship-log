# 🧬 实习日志 第38天｜线性代数与深度学习入门复盘

**日期：2025年9月26日**
**地点：首都医科大学生信实验室 / 纸笔 + JupyterLab（Mac）**

---

💡 今日完成的工作：

**1. 把“矩阵 = 变换”的直觉落到具体计算上**

* 回顾并实践了：矩阵把基向量映射到输出空间的列（`A = [c1 c2 ...]`），任意输入是列的线性组合；
* 手算并用 numpy 验证了简单例子：

  * $A=\begin{pmatrix}2&1\\1&2\end{pmatrix}$ 的特征值 $\lambda=3,1$ 及对应特征向量 $[1,1]^\top$、$[1,-1]^\top$；
  * 用 `np.linalg.eig` 检验手算结果，并把特征向量画在平面上直观展示变换前后效果；

**2. 理解 SVD / PCA 的直觉与实践**

* 学习并实践 SVD：把任意矩阵分解为 $U\Sigma V^\top$，理解为“先旋转 → 轴向缩放 → 再旋转”；
* 在小数据集上跑了 PCA（中心化后 `np.cov` → `np.linalg.eig` / `svd`），观察主成分方向与数据方差的对应关系，并用降到 2 维后的散点图比对原始分布；

**3. 把线性代数与神经网络连接起来**

* 回顾神经网络的基本模块：线性层 $y=Wx+b$（线性变换） + 激活函数（非线性）；举例说明仅堆叠线性层等价于单一线性变换，必须有非线性才能增加模型表达能力；
* 用 PyTorch（或手写 numpy）实现了一个非常小的两层网络示例（线性 → ReLU → 线性），在合成数据上看到了非线性如何把不可线性分割的数据变为可分；并画出隐藏层输出的分布以便理解“如何拐弯”；

**4. 实验：幂法（power iteration）和主特征向量**

* 手写幂法算法：随机向量投递、反复 $x_{k+1} = A x_k / \|A x_k\|$，观察收敛到主特征向量的过程；在稀疏大矩阵场景下讨论其优劣；
* 用小例子验证 PageRank 思路：把简单转移矩阵做幂迭代，收敛到稳定分布（特征值 1 的向量）。

---

📚 工作总结：

**技术能力提升方面**

* 把线性代数的抽象概念（列空间、特征向量、SVD）变成了可运行的代码与可视化，有助于加深直觉；
* 对“为什么需要非线性激活”有了实验级别的理解（通过小网络示例观察决策边界变化）；
* 学会并实践了两类求主特征向量的方法：直接分解（`eig`/`svd`）与迭代法（幂法），理解在大规模稀疏矩阵上的适用性差异。

**调试与工程化思路**

* 记录了实验环境与重现步骤（小 notebook），并把关键可视化图保存为 PNG，便于后续写笔记和讲解；
* 在写小网络与幂法时，注意数值稳定性（向量归一化、避免除以 0、对称矩阵处理等），并把这些小技巧记入笔记。

---

❗ 当前存在的问题 / 风险：

* 数值精度与奇异场景（接近重根/几何重数）会导致幂法收敛变慢或不稳定，需要引入正则化或 Lanczos 等更稳定方法；
* 对深度学习理论（如反向传播推导、泛化理论）尚处于初步理解阶段，需要系统化学习与实践；
* 在把线性代数直觉应用到真实数据（高维、生物数据）时还需考虑预处理（center/scale）与数值效率。

---

🎯 明日计划（可执行项）：

1. 把今天的 notebook 整理并推到 GitHub：包含特征值手算示例、SVD 可视化、幂法实现与小网络 demo；
2. 补强：系统学习反向传播的矩阵形式（用矩阵与链式法则写一遍），并做一份 1 页笔记；
3. 在 cellxgene 中加载一个真实的小数据集并尝试做 PCA 投影（把 PCA 结果和 cellxgene 的 UMAP/TSNE 做对比）；
4. 阅读并摘录 1 篇关于 SVD 在基因表达降噪/降维中应用的短文（写 200 字摘要）；
5. 若有空，基于今天的两层网络，把决策边界演示做成动画（不同 epoch 的边界变化），方便讲解“拐弯”概念。
