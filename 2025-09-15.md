🧬 实习日志 第30天｜深度学习回校学习后的复盘与今日问答记录
**日期：2025年9月15日**
**地点：首都医科大学生信实验室，iTerm2 / macOS**

---

💡 **今日完成的工作（概述）**

* 这些问题全部做了一遍：从“线性回归从零实现”到“PyTorch 的 DataLoader/`nn.Linear`/训练循环详解”，再到“`optimizer.zero_grad()` 的拼写 vs 功能区别”“梯度累加与 batch 大小的关系（等效大 batch）”，以及更深层的数值与硬件问题（浮点非结合性、GPU 为何擅长矩阵乘法、batch 不变性在 LLM 推理中造成的影响）。
* 在本地按提示组织并运行了两套代码：**从零实现的线性回归** 和 **高阶 API（TensorDataset + DataLoader + nn.Linear + optim）实现**，对比了训练流程与参数更新细节。
* 在本地笔记本中复读并注释了关键代码段（前向→loss→`zero_grad()`→`backward()`→`step()`），并把手算的小例子（单样本、单步）写成了可运行脚本以便验证梯度和参数更新。
* 上两周更新了 GitHub：在个人主页新增了一栏“深度学习学习日志”，把最近两周回校学习的笔记、今天的代码示例与训练记录整理上传（包含说明文档与运行指令）。

---

📈 **具体进度与实验记录**

* 代码：实现并运行了 `linear_reg_from_scratch.py`（手写数据生成、手动 sgd、显式 `loss.sum().backward()` + 除以 `batch_size` 更新）与 `linear_reg_with_dataloader.py`（TensorDataset + DataLoader + nn.MSELoss + torch.optim.SGD）。两者在少量 epoch 下均能把参数收敛到接近真值，验证了概率等价性（sum+除batch 与 mean 的等价性）。
* 调试：检查并理解了张量形状（`X:(batch, n)`、`w:(n,1)`、`b:(1,)`、`y:(batch,1)`）导致的维度错误原因，并把常见报错和排查步骤写入笔记（方便以后复用）。
* 训练循环学习：把训练循环的每一行拆解成数学表达 + 张量形状 + autograd 行为，写了一个小脚本打印 `y_hat`, `loss`, `model.weight.grad`, `model.weight.data` 的中间值，用于直观理解。
* 数值与硬件：复现了浮点非结合性的小例子（`(1e20 + -1e20) + 1` vs `1e20 + (-1e20 + 1)`），并在笔记中记录了为什么不同 batch 会触发不同内核实现（tiling、tensor cores、并行归约）从而改变单样本结果——初步把这类不变性问题写成了工程检查清单（记录：驱动/库版本、是否启用 Tensor Cores、是否使用混合精度、batch 策略）。

---

🎯 **今日收获**

* 把“从零实现”到“框架实现”两条思路彻底连通：清楚知道每一步在做什么、为什么要做（包括 `loss.sum()` vs `loss.mean()` 的等价性、`optimizer.zero_grad()` 的必要性与 `set_to_none=True` 的性能取舍）。
* 对 `nn.Linear`、`torch.nn.Parameter`、`requires_grad=True` 的语义更加清晰，知道如何在代码里检查与验证（`list(model.parameters())`、`param.requires_grad`、`param.grad`）。
* 熟练掌握了训练循环的调试套路：先看形状，再看 loss，再看 grad（是否为 None / 是否过大），最后看参数更新。
* 对数值稳定性与工程不确定性的认识更进一步：了解了为什么同一请求在不同 batch/负载下会产生差异（不是随机噪声，而是实现与舍入路径不同导致的确定性差异），并写出几条可行的缓解思路（固定 batch、禁用非确定性算子、导出统一推理引擎、在系统中区分“确定性”与“高吞吐”路径）。
* GitHub 页面有了“深度学习学习日志”板块，结构化记录有助于后续写作与申请材料支撑。

---

⚠️ **存在的问题 / 需要改进的地方**

* **实验覆盖不足**：还没有在不同硬件（不同 GPU 型号 / CPU / TPU）上做跨平台对比来量化 batch 不变性的影响；只在本机做了初步验证。
* **自动化测试缺失**：目前对“同一输入不同 batch 输出差异”的测试还没有自动化脚本（计划把对比测试写成单元/集成测试）。
* **对底层实现的理解不够深入**：比如 tensor cores 的具体累加精度、cuBLAS 不同算法切换策略、并行归约实现细节还需要读论文/源码或更深的资料学习。
* **代码独立性需要加强**：虽然能运行与调试示例，但有些代码仍依赖 ChatGPT 提供模板，后续要完全自主地写出更健壮、带异常处理和参数化的脚本。
* **文档还需润色**：GitHub 上的学习日志初稿需补充更多注释、运行环境说明（CUDA/cuDNN 版本等）以利复现。

---

🔜 **明日计划（短期可执行项）**

1. 把“batch 不变性对比测试”写成脚本（支持不同 batch、不同 precision、是否使用 tensor cores），并把输出差异自动化记录成 CSV。
2. 在 GitHub 的学习日志中补全“复现实验环境说明”（包括 `torch.__version__`、CUDA/cuDNN、GPU 型号），并把运行命令写成 `bash` 脚本提高可复现性。
3. 把训练循环模板扩展为支持**梯度累加**的版本（加入 `accum_steps` 参数）并做小规模实验验证等效性。
4. 阅读并总结 1–2 篇关于“deterministic inference / numerical reproducibility”的短文或博客（把要点写进笔记），为后续在推理服务中设计一致性策略做准备。
5. 明确一项长期目标：在 2 周内不依赖外部示例，独立完成一个小规模的深度学习实验（从数据生成、训练、日志、到 GitHub 文档），作为能力检验。
6. 文档：把上述学习点和脚本示例整理为 Markdown 并上传 GitHub，README 中写明如何重现实验、如何用不同 batch 做对比测试。
---

✍️ **个人反思**

回校学深度学习的两周把很多概念从模糊变清晰了，今天把理论、手算与实操连起来，感受很踏实。但仍要把“能复述”变成“能独立实现并优化”，下阶段重点放在自动化复现与底层原理的阅读上。
